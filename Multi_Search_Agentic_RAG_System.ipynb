{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV2dKEaRdsAYyOT+J2AevX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavun-KumarCH/AI-Enhanced-RAG-System-for-Automated-University-Course-Content-Generation/blob/main/Multi_Search_Agentic_RAG_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Search Agentic-RAG System\n",
        "### A system leveraging multiple knowledge retrieval tools for enhanced query processing.\n",
        "\n",
        "### Framework: LangChain  \n",
        "### LLM: OpenAI Models\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tools Used\n",
        "\n",
        "### 1.1. Wiki\n",
        "**Medium:** Wikipedia API or LangChain Wiki API integration  \n",
        "Wiki is used to retrieve factual and broad knowledge across a variety of domains. It is especially effective for general knowledge queries or historical data, where a detailed explanation or background information is necessary.\n",
        "\n",
        "### 1.2. arxiv\n",
        "**Medium:** arxiv API for research paper retrieval  \n",
        "arxiv provides access to scientific papers, useful for academic queries, technical topics, or research-related requests. This allows the system to respond with cutting-edge research or deep technical insights from recent publications.\n",
        "\n",
        "### 1.3. Retriever\n",
        "**Medium:** LangChain's Retriever Components  \n",
        "The retriever serves as a tool for fetching relevant documents based on the embeddings of the query. It operates across multiple sources (Wiki, arxiv, etc.) and ensures that the LLM has the most relevant context for generating responses.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. LLM and LangChain Integration\n",
        "\n",
        "### 2.1. LLM: OpenAI Models\n",
        "**Medium:** OpenAI GPT-4 and fine-tuned variations via LangChain  \n",
        "The LLM generates responses based on the documents retrieved by the retriever. It can summarize, interpret, and produce detailed answers, factoring in the specific knowledge drawn from the sources (Wiki, arxiv) used by the retriever.\n",
        "\n",
        "### 2.2. LangChain Framework\n",
        "**Medium:** Chains and Agents for multi-modal data flow  \n",
        "LangChain orchestrates the system’s flow, handling how user queries are split between different tools. It manages which tool (Wiki, arxiv, etc.) is used based on the nature of the question and ensures that the retrieved data is processed properly by the LLM.\n",
        "\n",
        "- **Agentic Behavior:**  \n",
        "  Dynamic agents enable the system to act autonomously, selecting the best retriever based on context and intelligently processing multi-tool requests.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Future Enhancements\n",
        "\n",
        "### 3.1. Integration of Additional Data Sources\n",
        "**Medium:** APIs or Web Scraping via LangChain  \n",
        "Potential additions include sources like Google Scholar, GitHub, or other specialized databases that can enhance the system’s versatility across domains.\n",
        "\n",
        "### 3.2. Fine-tuning for Specific Tasks\n",
        "**Medium:** Custom fine-tuning of the LLM  \n",
        "Fine-tuned models could be introduced to optimize the system for niche tasks, such as legal research or medical information retrieval."
      ],
      "metadata": {
        "id": "iq0w6FppRUC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "Wb-bZtZvRLFh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bNGXg_xdAybD"
      },
      "outputs": [],
      "source": [
        "!pip install --q arxiv wikipedia langchain_community langchain_openai faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Dependencies\n",
        "import os\n",
        "from langchain import hub\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
        "\n",
        "# Load Environment Variable\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "7yqc8vqbEUT3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "efC-DLbaKt-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wikipedia Tool\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results = 1, doc_content_chars_max = 200)\n",
        "\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "\n",
        "wiki_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wEXaeoi4GpcP",
        "outputId": "27ef7199-8b2a-46fa-b9fc-d0b5bb801b65"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Arxiv Tool\n",
        "# the website were all the research papers are being uploaded\n",
        "\n",
        "arxiv_api_wrapper = ArxivAPIWrapper()\n",
        "arxiv_tool = ArxivQueryRun(api_wrapper = arxiv_api_wrapper)\n",
        "\n",
        "arxiv_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nw1hphMDKHVF",
        "outputId": "f8aaf497-3cd0-4626-f68f-6686ee525be1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever Vectorstore\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "docs = loader.load()\n",
        "documents = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200).split_documents(docs)\n",
        "vector_db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
        "\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyWqRXdoHf4Y",
        "outputId": "e130553f-7a56-40f2-cb18-377006a087eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x78a690a45e10>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Retriever Tool\n",
        "retrival_tool = create_retriever_tool(retriever, \"langsmith_search\", \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")\n",
        "\n",
        "retrival_tool.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5pS3vm6gJUVz",
        "outputId": "440f6b4c-07c3-4c99-9c9c-ddb4ede4957b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'langsmith_search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools =  [wiki_tool, arxiv_tool, retrival_tool]"
      ],
      "metadata": {
        "id": "AC7cZom0KlOW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "Ihk6NsBVLs8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Agent\n",
        "\n",
        "# Initialize the LLM Model\n",
        "llm = ChatOpenAI(\n",
        "    model = \"gpt-3.5-turbo-0125\",\n",
        "    temperature = 0.3,\n",
        "    top_p = 0.8,\n",
        "    max_tokens = 400,\n",
        ")\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "# Intializing Agent\n",
        "Agent = create_openai_tools_agent(llm = llm,\n",
        "                                  tools = tools,\n",
        "                                  prompt = prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvJTg0YOLas_",
        "outputId": "d40c0903-f0c2-4ffa-b88d-3ae0f98f80d4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Executor"
      ],
      "metadata": {
        "id": "-J6qxzMfN5yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executor\n",
        "agent_executor = AgentExecutor(agent = Agent, tools = tools, verbose = True)"
      ],
      "metadata": {
        "id": "sDQVU8EdM0Fl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response's"
      ],
      "metadata": {
        "id": "FwU53XCKQ3XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Retriver Tool Invoke\n",
        "question = {\"input\" : \"Tell me About LangSmith and LangSmith Graph Agents\"}\n",
        "response = agent_executor.invoke(question)\n",
        "\n",
        "# Render using Markdown\n",
        "Markdown(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "x499NDLjOSON",
        "outputId": "0a3326b8-392b-4514-9c47-34ab4a285513"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `langsmith_search` with `{'query': 'LangSmith'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
            "\n",
            "Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãTracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on\n",
            "\n",
            "OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãTracing to LangSmith for LangChain usersThere is no need to use the LangSmith SDK directly if your application is built entirely on LangChain (either Python and JS).We've outlined a tracing guide specifically for LangChain users here.We provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
            "\n",
            "\"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators:\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM (Large Language Model) applications. It allows you to closely monitor and evaluate your application so you can ship quickly and with confidence. LangSmith works independently, and the use of LangChain is not necessary for its operation.\n",
            "\n",
            "LangSmith provides tools for tracing and evaluating applications built on LangChain. It offers features like creating datasets, examples, and evaluators to test and monitor the performance of your applications. You can log traces to LangSmith using various methods and APIs provided by the platform.\n",
            "\n",
            "If you have any specific questions or need more information about LangSmith or LangSmith Graph Agents, feel free to ask!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It allows you to closely monitor and evaluate your application so you can ship quickly and with confidence. LangSmith works independently, and the use of LangChain is not necessary for its operation.\n\nLangSmith provides tools for tracing and evaluating applications built on LangChain. It offers features like creating datasets, examples, and evaluators to test and monitor the performance of your applications. You can log traces to LangSmith using various methods and APIs provided by the platform.\n\nIf you have any specific questions or need more information about LangSmith or LangSmith Graph Agents, feel free to ask!"
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Wikipedia Tool Invoke\n",
        "question = {\"input\" : \"Tell me About GAN\"}\n",
        "response = agent_executor.invoke(question)\n",
        "\n",
        "# Render using Markdown\n",
        "Markdown(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "6O7aAcGjPs-J",
        "outputId": "a37fb861-46a6-4e71-84cc-b73592b33d1e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `wikipedia` with `{'query': 'Generative Adversarial Network'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Generative adversarial network\n",
            "Summary: A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intellige\u001b[0m\u001b[32;1m\u001b[1;3mA Generative Adversarial Network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. GANs consist of two neural networks, a generator, and a discriminator, which are trained simultaneously through adversarial training. The generator generates new data instances, while the discriminator evaluates the generated samples for authenticity. This process helps the generator improve its ability to create realistic data, leading to the generation of high-quality synthetic data.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A Generative Adversarial Network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. GANs consist of two neural networks, a generator, and a discriminator, which are trained simultaneously through adversarial training. The generator generates new data instances, while the discriminator evaluates the generated samples for authenticity. This process helps the generator improve its ability to create realistic data, leading to the generation of high-quality synthetic data."
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Arxiv Tool Invoke\n",
        "question = {\"input\" : \"What's the paper 1605.08386 about?\"}\n",
        "response = agent_executor.invoke(question)\n",
        "\n",
        "# Render using Markdown\n",
        "Markdown(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "m6LHYqpXQJIF",
        "outputId": "10226d3b-f713-464f-d6db-94f1510af195"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `arxiv` with `{'query': '1605.08386'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2016-05-26\n",
            "Title: Heat-bath random walks with Markov bases\n",
            "Authors: Caprice Stanley, Tobias Windisch\n",
            "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
            "allowed moves of arbitrary length. We show that the diameter of these graphs on\n",
            "fibers of a fixed integer matrix can be bounded from above by a constant. We\n",
            "then study the mixing behaviour of heat-bath random walks on these graphs. We\n",
            "also state explicit conditions on the set of moves so that the heat-bath random\n",
            "walk, a generalization of the Glauber dynamics, is an expander in fixed\n",
            "dimension.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper with identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper discusses the study of graphs on lattice points where edges are derived from a finite set of allowed moves of arbitrary length. The authors show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded by a constant. The paper also explores the mixing behavior of heat-bath random walks on these graphs and provides conditions on the set of moves for the heat-bath random walk to be an expander in a fixed dimension.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper with identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper discusses the study of graphs on lattice points where edges are derived from a finite set of allowed moves of arbitrary length. The authors show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded by a constant. The paper also explores the mixing behavior of heat-bath random walks on these graphs and provides conditions on the set of moves for the heat-bath random walk to be an expander in a fixed dimension."
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}